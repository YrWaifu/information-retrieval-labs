\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[russian]{babel}
\usepackage{graphicx}
\usepackage{float}

\begin{document}

% -------------------------
% Титульный лист
% -------------------------
\begin{titlepage}
\begin{center}
{\large Московский авиационный институт}\\
{\large (национальный исследовательский университет)}\\[3.5cm]

{\large Факультет информационных технологий и прикладной математики}\\[1.2cm]

{\large Кафедра вычислительной математики и программирования}\\[2.0cm]

{\large Лабораторные работы по курсу «Информационный поиск»}\\
\end{center}

\vspace{5.0cm} % если "Москва, 2025" съедет — уменьши до 5.5cm/5.0cm

\begin{flushright}
{%
\renewcommand{\arraystretch}{1.1}
\begin{tabular}{r@{:\ }l}
Студентка     & О.\,А.\,Титкова \\
Преподаватель & А.\,А.\,Кухтичев \\
Группа        & М8О-406Б-22 \\
Дата          & 24.12.2025 \\
Оценка        & \\
Подпись       & \\
\end{tabular}%
}
\end{flushright}

\vfill
\begin{center}
{\large Москва, 2025}
\end{center}
\end{titlepage}



% -------------------------
% Содержание (2-я страница)
% -------------------------
\tableofcontents
\newpage

\section{Цель работы}

Целью данной работы является изучение основных этапов построения простейшей поисковой системы. В рамках работы требуется сформировать корпус документов, подготовить его для последующей обработки, проанализировать статистические свойства текстов, а также реализовать индексирование и поиск по документам. Дополнительно необходимо рассмотреть примеры работы существующих поисковых систем и выявить их основные недостатки.

\section{Описание данных}

В качестве источника данных был выбран корпус статей из англоязычной версии Википедии, относящихся к тематике видеоигр. Данный источник предоставляет открытый программный интерфейс (API), позволяющий автоматически получать тексты статей и метаданные. Для формирования корпуса была выбрана корневая категория \texttt{Video games}, обход которой осуществлялся рекурсивно с ограничением глубины. Такой подход позволил получить тематически однородный корпус документов без выхода за пределы предметной области.

Сбор корпуса был реализован с использованием обхода категорий Википедии. Для каждой категории запрашивался список входящих в неё статей и подкатегорий, после чего подкатегории добавлялись в очередь обхода. Для каждой найденной статьи с использованием API запрашивался полный текст в виде плоского текста без вики-разметки. Документы малого объёма отфильтровывались на этапе загрузки, что позволило исключить служебные и неполные страницы.

В результате работы парсера был сформирован корпус из \textbf{30\,000 документов}. Каждый документ сохранён в отдельном текстовом файле и содержит основной текст статьи без служебной разметки. Для каждого документа также формируется файл метаданных, содержащий идентификатор документа, заголовок статьи, ссылку на источник и размер файла. Реализация загрузчика поддерживает возобновление работы после прерывания, что позволяет работать с корпусами большого размера без повторной загрузки уже обработанных документов.

Средний размер текстов в корпусе составляет \textbf{7057}, медианный размер — \textbf{3989}. Минимальный и максимальный размеры документов составляют \textbf{201} и \textbf{308614} символов соответственно.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{doc_sizes.png}
    \caption{Количество символов в документах}
    \label{fig:doc-sizes}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{text_example.png}
    \caption{Пример текста документа}
    \label{fig:text-example}
\end{figure}

\section{Закон Ципфа}

Для анализа статистических свойств корпуса был рассмотрен закон Ципфа, описывающий распределение частот слов в естественных языках. Перед выполнением анализа корпус был предварительно обработан. На этапе токенизации текст документов разбивался на последовательность токенов, при этом удалялись знаки пунктуации и приводился единый регистр. Далее применялся стемминг, позволяющий привести различные словоформы к общей основе и уменьшить размер словаря.

После нормализации текста для каждого уникального терма была подсчитана частота его вхождения во всём корпусе. Полученные данные были отсортированы по убыванию частоты, после чего каждому терму был присвоен ранг. На основе полученного распределения была построена зависимость частоты слова от его ранга.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{zipf.png}
    \caption{Закон Ципфа для корпуса статей о видеоиграх}
    \label{fig:zipf}
\end{figure}

\begin{verbatim}
rank    word    freq
1       the     2502562
2       and     1020405
3       of      932727
4       to      830274
5       in      713531
6       game    595295
7       wa      345416
8       for     336695
9       as      330005
\end{verbatim}

Анализ графика показывает, что распределение слов по частотам близко к линейному в логарифмических координатах, что соответствует закону Ципфа. Отклонения наблюдаются для наиболее частотных слов и в хвосте распределения, что характерно для реальных текстовых корпусов и подтверждает корректность этапов предварительной обработки текста.

\section{Примеры существующих поисковых систем}

Для анализа особенностей и ограничений современных поисковых систем были рассмотрены примеры поиска информации с использованием встроенного поиска Википедии и поисковой системы Google. В качестве запросов использовались термины, связанные с тематикой видеоигр.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{image.png}
    \caption{Пример поиска в Википедии}
    \label{fig:wiki-search}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{google.png}
    \caption{Пример поиска в Google}
    \label{fig:google-search}
\end{figure}

Анализ полученных результатов показывает, что существующие поисковые системы ориентированы на массового пользователя и популярные источники. При этом поиск по узкоспециализированным запросам часто требует дополнительного уточнения, а структура выдачи может содержать нерелевантные результаты или служебную информацию.

\section{Индексация и поиск}

Для обеспечения быстрого поиска по корпусу документов был реализован булев инвертированный индекс. Инвертированный индекс сопоставляет каждому терму список идентификаторов документов, в которых данный терм встречается. Такой подход позволяет выполнять поиск по логическим условиям без полного перебора всех документов корпуса.

В процессе индексирования каждый документ обрабатывается независимо. Из документа извлекается набор уникальных термов, что позволяет избежать дублирования идентификатора документа в постинг-листе при многократном вхождении одного и того же слова. Для каждого терма формируется отсортированный список идентификаторов документов, в которых он присутствует. В дальнейшем данные списки используются для выполнения операций булевого поиска.

Индекс сохраняется на диске в виде двух основных файлов: словаря терминов и бинарного файла постинг-листов. В словаре для каждого терма хранится количество документов, в которых он встречается, а также смещение и длина соответствующего постинг-листа в бинарном файле. Такая организация позволяет загружать в память только необходимые данные при обработке поискового запроса и обеспечивает масштабируемость решения при увеличении размера корпуса. Дополнительно сохраняется общее количество документов корпуса, используемое для корректной реализации оператора отрицания.

\begin{verbatim}
> head index/dict.tsv
term    df      offset  len
0-0     8       0       32
0-0-7   1       32      4
0-00-713655-2   1       36      4
0-00-717558-2   1       40      4
0-00-720907-x   3       44      12
0-007-24622-6   2       56      8
0-02-935671-7   1       64      4
0-049-28039-2   1       68      4
0-06-083305-x   1       72      4
\end{verbatim}

\begin{verbatim}
> wc -l index/dict.tsv
274105 index/dict.tsv

> ls -lh index/dict.tsv index/postings.bin index/maxdoc.txt
-rwxrwxrwx 1 user user 5.9M Dec 26 12:25 index/dict.tsv
-rwxrwxrwx 1 user user    6 Dec 26 12:25 index/maxdoc.txt
-rwxrwxrwx 1 user user  46M Dec 26 12:25 index/postings.bin

> cat index/maxdoc.txt
30000
\end{verbatim}

\section{Пример работы поисковой системы}

Для демонстрации работы реализованной поисковой системы были выполнены несколько булевых запросов с использованием логических операторов \texttt{AND}, \texttt{OR} и \texttt{NOT}, а также скобок для явного задания приоритета операций. Поисковый запрос разбирается и преобразуется в последовательность операций над постинг-листами. Операции пересечения, объединения и разности выполняются над отсортированными списками идентификаторов документов.

Для корректной реализации оператора отрицания используется множество всех документов корпуса, размер которого определяется на этапе индексирования. В результате выполнения запроса пользователю возвращается упорядоченный список идентификаторов документов, удовлетворяющих заданным условиям.

\begin{verbatim}
Loaded terms: 274104
Universe docs: 1..30000
Enter queries. Ctrl+D to exit.

nintendo
RESULTS 8905
1
...
30000
END

10-year AND NOT 10-year-old
RESULTS 46
3
...
29891
END

10-year-old OR 10-year
RESULTS 81
3
...
28337
29891
END

(10-minute OR 10-yard) AND 10-year
RESULTS 0
END
\end{verbatim}

\section{Статистика работы системы}

Для оценки эффективности реализованной системы была измерена производительность основных этапов обработки данных. В частности, было зафиксировано время построения индекса для полного корпуса.

\begin{verbatim}
> /usr/bin/time -p ./build_index --stems stems --out index

Processed docs: 500, pairs: 337955
Processed docs: 1000, pairs: 565153
...
Processed docs: 30000, pairs: 11974986
Index built.
Docs processed: 30000
maxDoc: 30000
Output: index/dict.tsv, postings.bin, maxdoc.txt
real 39.78
user 8.13
sys 3.67
\end{verbatim}

\section{Заключение}

В ходе выполнения данной работы были изучены основные принципы построения поисковых систем, включая сбор и подготовку корпуса документов, анализ статистических свойств текста, индексирование и реализацию булевого поиска. Был сформирован корпус большого объёма, обеспечивающий репрезентативность результатов и корректность статистического анализа.

Реализованная поисковая система демонстрирует корректную работу логических операций поиска и позволяет выполнять поиск по корпусу документов без полного перебора данных. Архитектура решения обеспечивает воспроизводимость результатов и возможность повторного использования индекса. В дальнейшем систему можно расширить за счёт внедрения методов ранжирования документов, позиционного поиска или дополнительных способов анализа текстов.

\end{document}
